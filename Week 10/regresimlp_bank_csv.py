# -*- coding: utf-8 -*-
"""RegresiMLP_bank_csv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/makhlufiaero338/tugas-machine-learning/blob/main/week10/RegresiMLP_bank_csv.ipynb
"""

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/bank.csv'
data = pd.read_csv(file_path, delimiter=';')

from google.colab import drive
drive.mount('/content/drive')

# Ganti path file sesuai lokasi file di Google Drive
file_path = '/content/drive/My Drive/bank.csv'

# Baca file
import pandas as pd
data = pd.read_csv(file_path, delimiter=';')

# Display basic information about the dataset
data_info = data.info()
data_head = data.head()

# Checking for missing values
missing_values = data.isnull().sum()

data_info, data_head, missing_values

# Reloading the dataset with the correct delimiter
data = pd.read_csv(file_path, delimiter=';')

# Display basic information and check for missing values after correction
data_info_corrected = data.info()
data_head_corrected = data.head()
missing_values_corrected = data.isnull().sum()

data_info_corrected, data_head_corrected, missing_values_corrected

import matplotlib.pyplot as plt

# Statistical summary of the dataset
data_description = data.describe(include='all')  # Sertakan semua kolom, baik numerik maupun kategorikal

# Distribution of the target variable (y)
plt.figure(figsize=(8, 5))
data['y'].value_counts().sort_index().plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Distribution of Target Variable (y)')
plt.xlabel('y (Target Variable)')
plt.ylabel('Count')
plt.xticks(rotation=0)  # Menjaga label sumbu x tetap horizontal
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

data_description

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Splitting features and target
X = data.drop(columns='y')  # Semua kolom kecuali target
y = data['y']  # Kolom target

# Encoding target variable (yes -> 1, no -> 0)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import OneHotEncoder

# One-Hot Encoding untuk fitur kategorikal
categorical_columns = data.select_dtypes(include=['object']).drop(columns='y').columns
encoder = OneHotEncoder(drop='first', sparse_output=False)  # Ubah 'sparse' menjadi 'sparse_output'
categorical_encoded = encoder.fit_transform(data[categorical_columns])

# Gabungkan data numerik dan kategorikal
numerical_columns = data.select_dtypes(include=['number']).columns
X_encoded = torch.tensor(
    pd.concat(
        [pd.DataFrame(categorical_encoded), data[numerical_columns].reset_index(drop=True)], axis=1
    ).values, dtype=torch.float32)

# Target variable
y_encoded = torch.tensor(y, dtype=torch.float32).unsqueeze(1)

# Splitting data into training and testing sets
X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = train_test_split(
    X_encoded, y_encoded, test_size=0.2, random_state=42
)

# Creating DataLoader for training and testing
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

# Example batch size (this will be varied later)
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define a basic Vanilla MLP model
class MLPBinaryClassification(nn.Module):
    def __init__(self, input_size, hidden_layers, activation_fn):
        super(MLPBinaryClassification, self).__init__()
        layers = []
        # Input layer to first hidden layer
        layers.append(nn.Linear(input_size, hidden_layers[0]))
        layers.append(activation_fn())
        # Hidden layers
        for i in range(1, len(hidden_layers)):
            layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))
            layers.append(activation_fn())
        # Output layer with sigmoid for binary classification
        layers.append(nn.Linear(hidden_layers[-1], 1))
        layers.append(nn.Sigmoid())
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

# Example initialization
input_size = X_train_tensor.shape[1]  # Number of features
hidden_layers = [32, 16]  # Example configuration
activation_fn = nn.ReLU  # Example activation function

mlp_model = MLPBinaryClassification(input_size, hidden_layers, activation_fn)
mlp_model

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import OneHotEncoder

# One-Hot Encoding untuk fitur kategorikal
categorical_columns = data.select_dtypes(include=['object']).drop(columns='y').columns
encoder = OneHotEncoder(drop='first', sparse_output=False)
categorical_encoded = encoder.fit_transform(data[categorical_columns])

# Gabungkan data numerik dan kategorikal
numerical_columns = data.select_dtypes(include=['number']).columns
X_encoded = torch.tensor(
    pd.concat(
        [pd.DataFrame(categorical_encoded), data[numerical_columns].reset_index(drop=True)], axis=1
    ).values, dtype=torch.float32)

# Target variable
y_encoded = torch.tensor(y, dtype=torch.float32).unsqueeze(1)

# Splitting data into training and testing sets
from sklearn.model_selection import train_test_split
X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = train_test_split(
    X_encoded, y_encoded, test_size=0.2, random_state=42
)

# Creating DataLoader for training and testing
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

# Example batch size (this will be varied later)
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define a basic Vanilla MLP model for Binary Classification
class MLPBinaryClassification(nn.Module):
    def __init__(self, input_size, hidden_layers, activation_fn):
        super(MLPBinaryClassification, self).__init__()
        layers = []
        # Input layer to first hidden layer
        layers.append(nn.Linear(input_size, hidden_layers[0]))
        layers.append(activation_fn())
        # Hidden layers
        for i in range(1, len(hidden_layers)):
            layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))
            layers.append(activation_fn())
        # Output layer with sigmoid for binary classification
        layers.append(nn.Linear(hidden_layers[-1], 1))
        layers.append(nn.Sigmoid())  # Sigmoid activation for binary classification
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

# Example initialization
input_size = X_train_tensor.shape[1]  # Number of features
hidden_layers = [32, 16]  # Example configuration
activation_fn = nn.ReLU  # Example activation function

mlp_model = MLPBinaryClassification(input_size, hidden_layers, activation_fn)

# Defining loss function and optimizer for binary classification
criterion = nn.BCELoss()  # Binary Cross-Entropy Loss
optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)  # Example learning rate

# Training function
def train_model(model, train_loader, criterion, optimizer, epochs=10):
    model.train()
    train_loss = []
    for epoch in range(epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        train_loss.append(epoch_loss / len(train_loader))
        print(f"Epoch {epoch+1}/{epochs}, Loss: {train_loss[-1]:.4f}")
    return train_loss

# Evaluation function
def evaluate_model(model, test_loader, criterion):
    model.eval()
    test_loss = 0.0
    correct_predictions = 0
    total_predictions = 0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            test_loss += loss.item()

            # Convert probabilities to binary values (0 or 1)
            predicted_classes = (predictions > 0.5).float()
            correct_predictions += (predicted_classes == y_batch).sum().item()
            total_predictions += y_batch.size(0)

    test_loss /= len(test_loader)
    accuracy = correct_predictions / total_predictions
    return test_loss, accuracy

# Training the model (example with 50 epochs)
epochs = 50
train_loss = train_model(mlp_model, train_loader, criterion, optimizer, epochs)

# Evaluating the model
test_loss, accuracy = evaluate_model(mlp_model, test_loader, criterion)
print(f"Test Loss (BCELoss): {test_loss:.4f}")
print(f"Test Accuracy: {accuracy * 100:.2f}%")

train_loss, test_loss, accuracy