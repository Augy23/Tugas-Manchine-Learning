# -*- coding: utf-8 -*-
"""tugas_1103210171_inception_resnet_densenet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/Augy23/d5f96bac120c909fb1fb052854ae73e1/tugas_1103210171_inception_resnet_densenet.ipynb

# Tutorial 5: Inception, ResNet and DenseNet

![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)


**Filled notebook:**
[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.ipynb)
[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.ipynb)   
**Pre-trained models:**
[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial5)
[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1zOgLKmYJ2V3uHz57nPUMY6tq15RmEtNg?usp=sharing)   
**Recordings:**
[![YouTube - Part 1](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%201&color=red)](https://youtu.be/vjSSyGxlczs)
[![YouTube - Part 2](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%202&color=red)](https://youtu.be/9yRXqYJDHr4)
[![YouTube - Part 3](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%203&color=red)](https://youtu.be/ELEqNwv9vkE)   
**JAX+Flax version:**
[![View on RTD](https://img.shields.io/static/v1.svg?logo=readthedocs&label=RTD&message=View%20On%20RTD&color=8CA1AF)](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial5/Inception_ResNet_DenseNet.html)   
**Author:** Phillip Lippe

<div class="alert alert-info">

**Note:** Interested in JAX? Check out our [JAX+Flax version](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial5/Inception_ResNet_DenseNet.html) of this tutorial!
</div>

Dalam tutorial ini, kita akan mengimplementasikan dan mendiskusikan berbagai varian arsitektur CNN (Convolutional Neural Network) modern. Selama beberapa tahun terakhir, banyak arsitektur berbeda telah diusulkan. Beberapa arsitektur yang paling berpengaruh, dan masih relevan hingga saat ini, adalah sebagai berikut:

GoogleNet/Inception architecture (pemenang ILSVRC 2014),
ResNet (pemenang ILSVRC 2015),
DenseNet (pemenang penghargaan makalah terbaik di CVPR 2017).
Semua arsitektur tersebut merupakan model state-of-the-art (terbaik di kelasnya) saat diusulkan, dan ide inti dari jaringan ini menjadi dasar bagi sebagian besar arsitektur state-of-the-art saat ini. Oleh karena itu, penting untuk memahami arsitektur ini secara mendalam dan mempelajari cara mengimplementasikannya.

Mari kita mulai dengan mengimpor pustaka standar kita.
"""

# Commented out IPython magic to ensure Python compatibility.
## Standard libraries
import os
import numpy as np
import random
from PIL import Image
from types import SimpleNamespace

## Imports for plotting
import matplotlib.pyplot as plt
# %matplotlib inline
from IPython.display import set_matplotlib_formats
set_matplotlib_formats('svg', 'pdf') # For export
import matplotlib
matplotlib.rcParams['lines.linewidth'] = 2.0
import seaborn as sns
sns.reset_orig()

## PyTorch
import torch
import torch.nn as nn
import torch.utils.data as data
import torch.optim as optim
# Torchvision
import torchvision
from torchvision.datasets import CIFAR10
from torchvision import transforms

"""Kita akan menggunakan fungsi set_seed yang sama seperti pada tutorial sebelumnya, serta variabel jalur (path) DATASET_PATH dan CHECKPOINT_PATH. Sesuaikan jalur ini jika diperlukan."""

# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)
DATASET_PATH = "../data"
# Path to the folder where the pretrained models are saved
CHECKPOINT_PATH = "../saved_models/tutorial5"

# Function for setting the seed
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
set_seed(42)

# Ensure that all operations are deterministic on GPU (if used) for reproducibility
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")

"""Kami juga memiliki model yang sudah dilatih sebelumnya (pretrained models) dan Tensorboards (akan dijelaskan lebih lanjut nanti) untuk tutorial ini, dan dapat diunduh di bawah ini."""

import urllib.request
from urllib.error import HTTPError
# Github URL where saved models are stored for this tutorial
base_url = "https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial5/"
# Files to download
pretrained_files = ["GoogleNet.ckpt", "ResNet.ckpt", "ResNetPreAct.ckpt", "DenseNet.ckpt",
                    "tensorboards/GoogleNet/events.out.tfevents.googlenet",
                    "tensorboards/ResNet/events.out.tfevents.resnet",
                    "tensorboards/ResNetPreAct/events.out.tfevents.resnetpreact",
                    "tensorboards/DenseNet/events.out.tfevents.densenet"]
# Create checkpoint path if it doesn't exist yet
os.makedirs(CHECKPOINT_PATH, exist_ok=True)

# For each file, check whether it already exists. If not, try downloading it.
for file_name in pretrained_files:
    file_path = os.path.join(CHECKPOINT_PATH, file_name)
    if "/" in file_name:
        os.makedirs(file_path.rsplit("/",1)[0], exist_ok=True)
    if not os.path.isfile(file_path):
        file_url = base_url + file_name
        print(f"Downloading {file_url}...")
        try:
            urllib.request.urlretrieve(file_url, file_path)
        except HTTPError as e:
            print("Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\n", e)

"""Sepanjang tutorial ini, kita akan melatih dan mengevaluasi model pada dataset CIFAR10. Hal ini memungkinkan Anda untuk membandingkan hasil yang diperoleh di sini dengan model yang telah Anda implementasikan pada tugas pertama. Seperti yang telah kita pelajari dari tutorial sebelumnya tentang inisialisasi, penting untuk memproses data terlebih dahulu dengan nilai rata-rata (mean) nol. Oleh karena itu, sebagai langkah pertama, kita akan menghitung nilai rata-rata (mean) dan standar deviasi (standard deviation) dari dataset CIFAR:"""

train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)
DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0,1,2))
DATA_STD = (train_dataset.data / 255.0).std(axis=(0,1,2))
print("Data mean", DATA_MEANS)
print("Data std", DATA_STD)

"""Kami akan menggunakan informasi ini untuk mendefinisikan modul transforms.Normalize yang akan menormalkan data kita sesuai kebutuhan. Selain itu, kami juga akan menggunakan augmentasi data selama pelatihan. Ini mengurangi risiko overfitting dan membantu CNN untuk lebih baik dalam generalizing. Secara khusus, kami akan menerapkan dua augmentasi acak.

Pertama, kami akan membalik setiap gambar secara horizontal dengan peluang 50% (transforms.RandomHorizontalFlip). Kelas objek biasanya tidak berubah ketika gambar dibalik, dan kami tidak mengharapkan informasi gambar bergantung pada orientasi horizontal. Namun, ini akan berbeda jika kita mencoba mendeteksi angka atau huruf dalam gambar, karena angka dan huruf memiliki orientasi tertentu.

Augmentasi kedua yang kami gunakan disebut transforms.RandomResizedCrop. Transformasi ini memotong gambar dalam rentang kecil, yang akhirnya mengubah rasio aspek, lalu mengubah ukuran gambar kembali ke ukuran semula. Oleh karena itu, nilai piksel yang sebenarnya berubah, sementara konten atau semantik keseluruhan gambar tetap sama.

Kami akan membagi dataset pelatihan secara acak menjadi set pelatihan dan validasi. Set validasi akan digunakan untuk menentukan early stopping. Setelah menyelesaikan pelatihan, kami menguji model pada set tes CIFAR.
"""

test_transform = transforms.Compose([transforms.ToTensor(),
                                     transforms.Normalize(DATA_MEANS, DATA_STD)
                                     ])
# For training, we add some augmentation. Networks are too powerful and would overfit.
train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),
                                      transforms.RandomResizedCrop((32,32), scale=(0.8,1.0), ratio=(0.9,1.1)),
                                      transforms.ToTensor(),
                                      transforms.Normalize(DATA_MEANS, DATA_STD)
                                     ])
# Loading the training dataset. We need to split it into a training and validation part
# We need to do a little trick because the validation set should not use the augmentation.
train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)
val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)
set_seed(42)
train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])
set_seed(42)
_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])

# Loading the test set
test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)

# We define a set of data loaders that we can use for various purposes later.
train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)
val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)
test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)

"""Untuk memverifikasi bahwa normalisasi kita berfungsi dengan baik, kita dapat mencetak nilai rata-rata (mean) dan standar deviasi (standard deviation) dari satu batch data. Nilai rata-rata seharusnya mendekati 0 dan standar deviasi mendekati 1 untuk setiap saluran (channel)."""

imgs, _ = next(iter(train_loader))
print("Batch mean", imgs.mean(dim=[0,2,3]))
print("Batch std", imgs.std(dim=[0,2,3]))

"""Terakhir, mari kita visualisasikan beberapa gambar dari set pelatihan, serta bagaimana tampilan mereka setelah augmentasi data acak.


"""

NUM_IMAGES = 4
images = [train_dataset[idx][0] for idx in range(NUM_IMAGES)]
orig_images = [Image.fromarray(train_dataset.data[idx]) for idx in range(NUM_IMAGES)]
orig_images = [test_transform(img) for img in orig_images]

img_grid = torchvision.utils.make_grid(torch.stack(images + orig_images, dim=0), nrow=4, normalize=True, pad_value=0.5)
img_grid = img_grid.permute(1, 2, 0)

plt.figure(figsize=(8,8))
plt.title("Augmentation examples on CIFAR10")
plt.imshow(img_grid)
plt.axis('off')
plt.show()
plt.close()

"""## PyTorch Lightning



Dalam notebook ini dan beberapa notebook berikutnya, kita akan menggunakan pustaka [PyTorch Lightning](https://www.pytorchlightning.ai/). PyTorch Lightning adalah kerangka kerja yang menyederhanakan kode yang diperlukan untuk melatih, mengevaluasi, dan menguji model di PyTorch. PyTorch Lightning juga menangani proses logging ke [TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html), yaitu alat visualisasi untuk eksperimen ML, serta secara otomatis menyimpan checkpoint model dengan sedikit kode tambahan dari pihak kita. Hal ini sangat membantu karena kita ingin fokus pada implementasi berbagai arsitektur model tanpa menghabiskan banyak waktu pada kode tambahan.

Perlu dicatat bahwa pada saat penulisan/pengajaran ini, kerangka kerja ini telah dirilis dalam versi 1.8. Versi di masa depan mungkin memiliki antarmuka yang sedikit berubah, sehingga mungkin tidak sepenuhnya kompatibel dengan kode (kami akan mencoba untuk memperbarui sejauh mungkin).

Sekarang, kita akan mengambil langkah pertama dalam PyTorch Lightning dan terus menjelajahi kerangka kerja ini di tutorial berikutnya. Pertama, kita impor pustaka tersebut:
"""

# PyTorch Lightning
try:
    import pytorch_lightning as pl
except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary
    !pip install --quiet pytorch-lightning>=1.5
    import pytorch_lightning as pl

"""PyTorch Lightning menyediakan banyak fungsi yang berguna, salah satunya adalah fungsi untuk menetapkan seed (nilai acak awal)."""

# Setting the seed
pl.seed_everything(42)

"""

Dengan demikian, di masa depan, kita tidak perlu lagi mendefinisikan fungsi set_seed kita sendiri.

Di PyTorch Lightning, kita mendefinisikan pl.LightningModule (menurunkan dari torch.nn.Module) yang mengorganisasi kode kita menjadi 5 bagian utama:

Inisialisasi (__init__): Di sini, kita membuat semua parameter dan model yang diperlukan.
Optimizers (configure_optimizers): Di sini, kita membuat optimizers, scheduler learning rate, dan sebagainya.
Training Loop (training_step): Di sini, kita hanya perlu mendefinisikan perhitungan loss untuk satu batch. Loop seperti optimizer.zero_grad(), loss.backward(), dan optimizer.step(), serta operasi logging/saving, dilakukan di latar belakang.
Validation Loop (validation_step): Serupa dengan pelatihan, kita hanya perlu mendefinisikan apa yang harus dilakukan per langkah validasi.
Test Loop (test_step): Sama seperti validasi, tetapi pada set pengujian (test set).
Oleh karena itu, kita tidak menyembunyikan kode PyTorch, tetapi hanya mengorganisasikannya dan mendefinisikan beberapa operasi default yang umum digunakan. Jika Anda perlu mengubah sesuatu di loop pelatihan/validasi/pengujian, ada banyak fungsi yang dapat Anda overwrite (lihat [docs](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html) untuk detailnya).

Sekarang kita dapat melihat contoh bagaimana LightningModule untuk melatih CNN terlihat."""

class CIFARModule(pl.LightningModule):

    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):
        """
        Inputs:
            model_name - Name of the model/CNN to run. Used for creating the model (see function below)
            model_hparams - Hyperparameters for the model, as dictionary.
            optimizer_name - Name of the optimizer to use. Currently supported: Adam, SGD
            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.
        """
        super().__init__()
        # Exports the hyperparameters to a YAML file, and create "self.hparams" namespace
        self.save_hyperparameters()
        # Create model
        self.model = create_model(model_name, model_hparams)
        # Create loss module
        self.loss_module = nn.CrossEntropyLoss()
        # Example input for visualizing the graph in Tensorboard
        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)

    def forward(self, imgs):
        # Forward function that is run when visualizing the graph
        return self.model(imgs)

    def configure_optimizers(self):
        # We will support Adam or SGD as optimizers.
        if self.hparams.optimizer_name == "Adam":
            # AdamW is Adam with a correct implementation of weight decay (see here for details: https://arxiv.org/pdf/1711.05101.pdf)
            optimizer = optim.AdamW(
                self.parameters(), **self.hparams.optimizer_hparams)
        elif self.hparams.optimizer_name == "SGD":
            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)
        else:
            assert False, f"Unknown optimizer: \"{self.hparams.optimizer_name}\""

        # We will reduce the learning rate by 0.1 after 100 and 150 epochs
        scheduler = optim.lr_scheduler.MultiStepLR(
            optimizer, milestones=[100, 150], gamma=0.1)
        return [optimizer], [scheduler]

    def training_step(self, batch, batch_idx):
        # "batch" is the output of the training data loader.
        imgs, labels = batch
        preds = self.model(imgs)
        loss = self.loss_module(preds, labels)
        acc = (preds.argmax(dim=-1) == labels).float().mean()

        # Logs the accuracy per epoch to tensorboard (weighted average over batches)
        self.log('train_acc', acc, on_step=False, on_epoch=True)
        self.log('train_loss', loss)
        return loss  # Return tensor to call ".backward" on

    def validation_step(self, batch, batch_idx):
        imgs, labels = batch
        preds = self.model(imgs).argmax(dim=-1)
        acc = (labels == preds).float().mean()
        # By default logs it per epoch (weighted average over batches)
        self.log('val_acc', acc)

    def test_step(self, batch, batch_idx):
        imgs, labels = batch
        preds = self.model(imgs).argmax(dim=-1)
        acc = (labels == preds).float().mean()
        # By default logs it per epoch (weighted average over batches), and returns it afterwards
        self.log('test_acc', acc)

"""Kita melihat bahwa kode yang digunakan terorganisir dan jelas, yang sangat membantu jika orang lain mencoba memahami kode Anda.

Bagian penting lainnya dari PyTorch Lightning adalah konsep callbacks. Callbacks adalah fungsi yang terpisah yang berisi logika tidak penting (non-essential logic) dari LightningModule Anda. Biasanya, mereka dipanggil setelah menyelesaikan satu epoch pelatihan, tetapi juga dapat memengaruhi bagian lain dari training loop. Misalnya, kita akan menggunakan dua callback bawaan berikut:

LearningRateMonitor: Menambahkan learning rate saat ini ke TensorBoard, sehingga membantu memverifikasi apakah learning rate scheduler bekerja dengan benar.
ModelCheckpoint: Memungkinkan Anda menyesuaikan rutinitas penyimpanan checkpoint. Misalnya, Anda dapat menentukan berapa banyak checkpoint yang disimpan, kapan disimpan, metrik apa yang diperhatikan, dan sebagainya.

Kita impor kedua callback ini di bawah ini:
"""

# Callbacks
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint

"""Untuk memungkinkan menjalankan beberapa model yang berbeda dengan modul Lightning yang sama, kita mendefinisikan fungsi di bawah ini yang memetakan nama model ke kelas model. Pada tahap ini, kamus (dictionary) model_dict masih kosong, tetapi kita akan mengisinya sepanjang notebook ini dengan model-model baru kita."""

model_dict = {}

def create_model(model_name, model_hparams):
    if model_name in model_dict:
        return model_dict[model_name](**model_hparams)
    else:
        assert False, f"Unknown model name \"{model_name}\". Available models are: {str(model_dict.keys())}"

"""Demikian pula, untuk menggunakan fungsi aktivasi sebagai hiperparameter lain dalam model kita, kita mendefinisikan sebuah kamus ("name to function") di bawah ini:"""

act_fn_by_name = {
    "tanh": nn.Tanh,
    "relu": nn.ReLU,
    "leakyrelu": nn.LeakyReLU,
    "gelu": nn.GELU
}

"""

Jika kita mengirimkan kelas atau objek langsung sebagai argumen ke Lightning module, kita tidak bisa memanfaatkan fitur otomatis dari PyTorch Lightning untuk menyimpan dan memuat hiperparameter.

Selain Lightning module, modul kedua yang paling penting di PyTorch Lightning adalah Trainer. Trainer bertanggung jawab untuk menjalankan langkah-langkah pelatihan yang didefinisikan dalam Lightning module dan melengkapi kerangka kerja tersebut. Sama seperti Lightning module, Anda dapat menimpa (override) bagian kunci mana pun yang tidak ingin diotomatisasi, tetapi pengaturan bawaan biasanya adalah praktik terbaik untuk digunakan. Untuk gambaran lengkap, lihat [documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html).

Fungsi paling penting yang digunakan di bawah ini adalah:

trainer.fit:
Menerima sebagai input sebuah Lightning module, dataset pelatihan, dan (opsional) dataset validasi.
Fungsi ini melatih modul pada dataset pelatihan dengan validasi sesekali (secara default sekali per epoch, tetapi dapat diubah).

trainer.test:
Menerima sebagai input model dan dataset tempat kita ingin melakukan pengujian.
Mengembalikan metrik pengujian pada dataset tersebut.
Selama pelatihan dan pengujian, kita tidak perlu khawatir tentang hal-hal seperti mengatur model ke mode evaluasi (model.eval()) karena semuanya dilakukan secara otomatis. Berikut adalah cara kita mendefinisikan fungsi pelatihan untuk model kita:"""

def train_model(model_name, save_name=None, **kwargs):
    """
    Inputs:
        model_name - Name of the model you want to run. Is used to look up the class in "model_dict"
        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.
    """
    if save_name is None:
        save_name = model_name

    # Create a PyTorch Lightning trainer with the generation callback
    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),                          # Where to save models
                         accelerator="gpu" if str(device).startswith("cuda") else "cpu",                     # We run on a GPU (if possible)
                         devices=1,                                                                          # How many GPUs/CPUs we want to use (1 is enough for the notebooks)
                         max_epochs=180,                                                                     # How many epochs to train for if no patience is set
                         callbacks=[ModelCheckpoint(save_weights_only=True, mode="max", monitor="val_acc"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer
                                    LearningRateMonitor("epoch")],                                           # Log learning rate every epoch
                         enable_progress_bar=True)                                                           # Set to False if you do not want a progress bar
    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard
    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need

    # Check whether pretrained model exists. If yes, load it and skip training
    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + ".ckpt")
    if os.path.isfile(pretrained_filename):
        print(f"Found pretrained model at {pretrained_filename}, loading...")
        model = CIFARModule.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters
    else:
        pl.seed_everything(42) # To be reproducable
        model = CIFARModule(model_name=model_name, **kwargs)
        trainer.fit(model, train_loader, val_loader)
        model = CIFARModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training

    # Test best model on validation and test set
    val_result = trainer.test(model, val_loader, verbose=False)
    test_result = trainer.test(model, test_loader, verbose=False)
    result = {"test": test_result[0]["test_acc"], "val": val_result[0]["test_acc"]}

    return model, result

"""Akhirnya, kita dapat fokus pada Convolutional Neural Networks (CNN) yang ingin kita implementasikan hari ini: GoogleNet, ResNet, dan DenseNet.

## Inception



[GoogleNet](https://arxiv.org/abs/1409.4842), yang diajukan pada tahun 2014, memenangkan ImageNet Challenge karena penggunaan modul Inception. Secara umum, kita akan fokus pada konsep Inception dalam tutorial ini, bukan pada rincian GoogleNet, karena berdasarkan Inception, ada banyak karya lanjutan ([Inception-v2](https://arxiv.org/abs/1512.00567), [Inception-v3](https://arxiv.org/abs/1512.00567), [Inception-v4](https://arxiv.org/abs/1602.07261), [Inception-ResNet](https://arxiv.org/abs/1602.07261),...). Karya lanjutan ini fokus pada peningkatan efisiensi dan memungkinkan jaringan Inception yang sangat dalam. Namun, untuk pemahaman dasar, cukup dengan melihat blok Inception asli.

Sebuah blok Inception menerapkan empat blok konvolusi secara terpisah pada peta fitur yang sama: konvolusi 1x1, 3x3, dan 5x5, serta operasi max pooling. Hal ini memungkinkan jaringan untuk melihat data yang sama dengan berbagai ukuran jangkauan penerimaan (receptive fields). Tentu saja, mempelajari konvolusi 5x5 saja secara teori akan lebih kuat. Namun, ini tidak hanya membutuhkan lebih banyak komputasi dan memori, tetapi juga cenderung lebih mudah untuk overfit. Blok Inception secara keseluruhan terlihat seperti di bawah ini (figure credit - [Szegedy et al.](https://arxiv.org/abs/1409.4842)):

<center width="100%"><img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/inception_block.svg?raw=1" style="display: block; margin-left: auto; margin-right: auto;" width="500px"/></center>

Konvolusi 1x1 tambahan sebelum konvolusi 3x3 dan 5x5 digunakan untuk reduksi dimensi. Ini sangat penting karena peta fitur dari semua cabang akan digabungkan setelahnya, dan kita tidak ingin terjadinya ledakan ukuran fitur. Karena konvolusi 5x5 memerlukan komputasi yang 25 kali lebih mahal daripada konvolusi 1x1, kita dapat menghemat banyak perhitungan dan parameter dengan mengurangi dimensi terlebih dahulu sebelum konvolusi besar.

Sekarang kita dapat mencoba untuk mengimplementasikan Inception Block sendiri:
"""

class InceptionBlock(nn.Module):

    def __init__(self, c_in, c_red : dict, c_out : dict, act_fn):
        """
        Inputs:
            c_in - Number of input feature maps from the previous layers
            c_red - Dictionary with keys "3x3" and "5x5" specifying the output of the dimensionality reducing 1x1 convolutions
            c_out - Dictionary with keys "1x1", "3x3", "5x5", and "max"
            act_fn - Activation class constructor (e.g. nn.ReLU)
        """
        super().__init__()

        # 1x1 convolution branch
        self.conv_1x1 = nn.Sequential(
            nn.Conv2d(c_in, c_out["1x1"], kernel_size=1),
            nn.BatchNorm2d(c_out["1x1"]),
            act_fn()
        )

        # 3x3 convolution branch
        self.conv_3x3 = nn.Sequential(
            nn.Conv2d(c_in, c_red["3x3"], kernel_size=1),
            nn.BatchNorm2d(c_red["3x3"]),
            act_fn(),
            nn.Conv2d(c_red["3x3"], c_out["3x3"], kernel_size=3, padding=1),
            nn.BatchNorm2d(c_out["3x3"]),
            act_fn()
        )

        # 5x5 convolution branch
        self.conv_5x5 = nn.Sequential(
            nn.Conv2d(c_in, c_red["5x5"], kernel_size=1),
            nn.BatchNorm2d(c_red["5x5"]),
            act_fn(),
            nn.Conv2d(c_red["5x5"], c_out["5x5"], kernel_size=5, padding=2),
            nn.BatchNorm2d(c_out["5x5"]),
            act_fn()
        )

        # Max-pool branch
        self.max_pool = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),
            nn.Conv2d(c_in, c_out["max"], kernel_size=1),
            nn.BatchNorm2d(c_out["max"]),
            act_fn()
        )

    def forward(self, x):
        x_1x1 = self.conv_1x1(x)
        x_3x3 = self.conv_3x3(x)
        x_5x5 = self.conv_5x5(x)
        x_max = self.max_pool(x)
        x_out = torch.cat([x_1x1, x_3x3, x_5x5, x_max], dim=1)
        return x_out

"""Arsitektur GoogleNet terdiri dari beberapa Inception block yang disusun secara berturut-turut dengan max pooling sesekali untuk mengurangi tinggi dan lebar peta fitur. GoogleNet asli dirancang untuk ukuran gambar ImageNet (224x224 piksel) dan memiliki hampir 7 juta parameter. Karena kita melatih model pada CIFAR10 dengan ukuran gambar 32x32, kita tidak memerlukan arsitektur yang begitu besar, dan sebaliknya, kita menggunakan versi yang lebih ringan. Jumlah saluran untuk reduksi dimensi dan keluaran per filter (1x1, 3x3, 5x5, dan max pooling) perlu ditentukan secara manual dan dapat diubah jika diinginkan. Intuisi umumnya adalah untuk memiliki filter terbanyak pada konvolusi 3x3, karena filter ini cukup kuat untuk mempertimbangkan konteks sambil membutuhkan hampir sepertiga dari jumlah parameter konvolusi 5x5."""

class GoogleNet(nn.Module):

    def __init__(self, num_classes=10, act_fn_name="relu", **kwargs):
        super().__init__()
        self.hparams = SimpleNamespace(num_classes=num_classes,
                                       act_fn_name=act_fn_name,
                                       act_fn=act_fn_by_name[act_fn_name])
        self._create_network()
        self._init_params()

    def _create_network(self):
        # A first convolution on the original image to scale up the channel size
        self.input_net = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            self.hparams.act_fn()
        )
        # Stacking inception blocks
        self.inception_blocks = nn.Sequential(
            InceptionBlock(64, c_red={"3x3": 32, "5x5": 16}, c_out={"1x1": 16, "3x3": 32, "5x5": 8, "max": 8}, act_fn=self.hparams.act_fn),
            InceptionBlock(64, c_red={"3x3": 32, "5x5": 16}, c_out={"1x1": 24, "3x3": 48, "5x5": 12, "max": 12}, act_fn=self.hparams.act_fn),
            nn.MaxPool2d(3, stride=2, padding=1),  # 32x32 => 16x16
            InceptionBlock(96, c_red={"3x3": 32, "5x5": 16}, c_out={"1x1": 24, "3x3": 48, "5x5": 12, "max": 12}, act_fn=self.hparams.act_fn),
            InceptionBlock(96, c_red={"3x3": 32, "5x5": 16}, c_out={"1x1": 16, "3x3": 48, "5x5": 16, "max": 16}, act_fn=self.hparams.act_fn),
            InceptionBlock(96, c_red={"3x3": 32, "5x5": 16}, c_out={"1x1": 16, "3x3": 48, "5x5": 16, "max": 16}, act_fn=self.hparams.act_fn),
            InceptionBlock(96, c_red={"3x3": 32, "5x5": 16}, c_out={"1x1": 32, "3x3": 48, "5x5": 24, "max": 24}, act_fn=self.hparams.act_fn),
            nn.MaxPool2d(3, stride=2, padding=1),  # 16x16 => 8x8
            InceptionBlock(128, c_red={"3x3": 48, "5x5": 16}, c_out={"1x1": 32, "3x3": 64, "5x5": 16, "max": 16}, act_fn=self.hparams.act_fn),
            InceptionBlock(128, c_red={"3x3": 48, "5x5": 16}, c_out={"1x1": 32, "3x3": 64, "5x5": 16, "max": 16}, act_fn=self.hparams.act_fn)
        )
        # Mapping to classification output
        self.output_net = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, self.hparams.num_classes)
        )

    def _init_params(self):
        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(
                    m.weight, nonlinearity=self.hparams.act_fn_name)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.input_net(x)
        x = self.inception_blocks(x)
        x = self.output_net(x)
        return x

"""Sekarang, kita dapat mengintegrasikan model kita ke dalam model dictionary yang telah kita definisikan sebelumnya:"""

model_dict["GoogleNet"] = GoogleNet

"""Pelatihan model ditangani oleh PyTorch Lightning, dan kita hanya perlu mendefinisikan perintah untuk memulai pelatihan. Perhatikan bahwa kita melatih model selama hampir 200 epoch, yang memakan waktu kurang dari satu jam pada GPU default Snellius (NVIDIA A100). Kami menyarankan untuk menggunakan model yang sudah disimpan dan melatih model Anda sendiri jika Anda tertarik."""

googlenet_model, googlenet_results = train_model(model_name="GoogleNet",
                                                 model_hparams={"num_classes": 10,
                                                                "act_fn_name": "relu"},
                                                 optimizer_name="Adam",
                                                 optimizer_hparams={"lr": 1e-3,
                                                                    "weight_decay": 1e-4})

"""Kita akan membandingkan hasilnya nanti di notebook, tetapi kita sudah dapat mencetak hasilnya di sini untuk sekilas pandang pertama:"""

print("GoogleNet Results", googlenet_results)

"""### Tensorboard log

Keuntungan tambahan dari PyTorch Lightning adalah pencatatan otomatis ke TensorBoard. Untuk memberikan gambaran yang lebih baik tentang apa yang dapat dilakukan dengan TensorBoard, kita bisa melihat papan yang dihasilkan oleh PyTorch Lightning saat melatih GoogleNet. TensorBoard menyediakan fungsionalitas inline untuk Jupyter notebooks, dan kita akan menggunakannya di sini:
"""

# Commented out IPython magic to ensure Python compatibility.
# Load tensorboard extension
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!
# %tensorboard --logdir ../saved_models/tutorial5/tensorboards/GoogleNet/

"""<center width="100%"><img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/tensorboard_screenshot_GoogleNet.png?raw=1" width="1000px"></center>

TensorBoard diorganisir dalam beberapa tab. Tab utama adalah tab scalar yang memungkinkan kita untuk mencatat perkembangan angka tunggal. Sebagai contoh, kita dapat memplot grafik training loss, akurasi, laju pembelajaran, dan sebagainya. Jika kita melihat akurasi pelatihan atau validasi, kita benar-benar bisa melihat dampak dari penggunaan learning rate scheduler. Mengurangi laju pembelajaran memberikan peningkatan kinerja pelatihan yang baik untuk model kita. Demikian pula, saat melihat training loss, kita melihat penurunan yang tajam pada titik tersebut. Namun, angka tinggi pada set pelatihan dibandingkan dengan validasi menunjukkan bahwa model kita mengalami overfitting, yang tak terhindarkan pada jaringan sebesar ini.

Tab lain yang menarik di TensorBoard adalah tab graph. Tab ini menunjukkan arsitektur jaringan kita yang diorganisir berdasarkan blok bangunan dari input ke output. Pada dasarnya, ini menunjukkan operasi yang dilakukan dalam langkah forward dari CIFARModule. Klik ganda pada sebuah modul untuk membukanya. Jangan ragu untuk mengeksplorasi arsitektur dari perspektif yang berbeda. Visualisasi grafik ini sering kali membantu Anda memverifikasi bahwa model Anda benar-benar melakukan apa yang seharusnya, dan Anda tidak melewatkan lapisan manapun dalam grafik komputasi.

## ResNet

The [ResNet](https://arxiv.org/abs/1512.03385) paper is one of the  [most cited AI papers](https://www.natureindex.com/news-blog/google-scholar-reveals-most-influential-papers-research-citations-twenty-twenty), and has been the foundation for neural networks with more than 1,000 layers. Despite its simplicity, the idea of residual connections is highly effective as it supports stable gradient propagation through the network. Instead of modeling $x_{l+1}=F(x_{l})$, we model $x_{l+1}=x_{l}+F(x_{l})$ where $F$ is a non-linear mapping (usually a sequence of NN modules likes convolutions, activation functions, and normalizations). If we do backpropagation on such residual connections, we obtain:

Makalah [ResNet](https://arxiv.org/abs/1512.03385) adalah [salah satu makalah AI yang paling banyak disitasi](https://www.natureindex.com/news-blog/google-scholar-reveals-most-influential-papers-research-citations-twenty-twenty), dan telah menjadi dasar bagi jaringan saraf dengan lebih dari 1.000 lapisan. Meskipun kesederhanaannya, ide dari residual connections sangat efektif karena mendukung propagasi gradien yang stabil melalui jaringan. Alih-alih memodelkan $x_{l+1}=F(x_{l})$, model kami $x_{l+1}=x_{l}+F(x_{l})$ dimana $F$ adalah pemetaan non-linier (biasanya serangkaian modul jaringan saraf seperti konvolusi, fungsi aktivasi, dan normalisasi). Jika kita melakukan backpropagation pada koneksi residual seperti itu, kita akan memperoleh:

$$\frac{\partial x_{l+1}}{\partial x_{l}} = \mathbf{I} + \frac{\partial F(x_{l})}{\partial x_{l}}$$


Bias terhadap matriks identitas menjamin propagasi gradien yang stabil yang kurang dipengaruhi oleh $F$F itu sendiri. Telah ada banyak varian dari ResNet yang diajukan, yang sebagian besar berkaitan dengan fungsi $F$, atau operasi yang diterapkan pada jumlahnya. Dalam tutorial ini, kita melihat dua di antaranya: blok ResNet asli, dan blok [Pre-Activation ResNet block](https://arxiv.org/abs/1603.05027). Kami membandingkan secara visual blok-blok ini di bawah (figure credit - [He et al.](https://arxiv.org/abs/1603.05027)):

<center width="100%"><img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/resnet_block.svg?raw=1" style="display: block; margin-left: auto; margin-right: auto;" width="300px"/></center>



Blok ResNet asli menerapkan fungsi aktivasi non-linier, biasanya ReLU, setelah koneksi shortcut. Sebaliknya, blok Pre-Activation ResNet menerapkan non-linieritas di awal $F$. Keduanya memiliki kelebihan dan kekurangan masing-masing. Namun, untuk jaringan yang sangat dalam, Pre-Activation ResNet telah terbukti memberikan performa yang lebih baik karena aliran gradien dijamin memiliki matriks identitas seperti yang dihitung sebelumnya, dan tidak terpengaruh oleh fungsi aktivasi non-linier yang diterapkan padanya. Untuk perbandingan, dalam notebook ini, kita akan mengimplementasikan kedua jenis ResNet sebagai jaringan dangkal.

Mari kita mulai dengan blok ResNet asli. Visualisasi di atas sudah menunjukkan lapisan apa saja yang ada dalam $F$. Salah satu kasus khusus yang harus kita tangani adalah ketika kita ingin mengurangi dimensi gambar dalam hal lebar dan tinggi. Blok ResNet dasar mengharuskan $F(x_{l})$ memiliki bentuk yang sama dengan $x_{l}$. Oleh karena itu, kita perlu mengubah dimensi $x_{l}$ juga sebelum ditambahkan ke $F(x_{l})$. Implementasi asli menggunakan pemetaan identitas dengan stride 2 dan menambahkan dimensi fitur tambahan dengan 0. Namun, implementasi yang lebih umum adalah menggunakan konvolusi 1x1 dengan stride 2 karena ini memungkinkan kita untuk mengubah dimensi fitur sambil tetap efisien dalam biaya parameter dan komputasi. Kode untuk blok ResNet cukup sederhana, dan ditunjukkan di bawah:
"""

class ResNetBlock(nn.Module):

    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):
        """
        Inputs:
            c_in - Number of input features
            act_fn - Activation class constructor (e.g. nn.ReLU)
            subsample - If True, we want to apply a stride inside the block and reduce the output shape by 2 in height and width
            c_out - Number of output features. Note that this is only relevant if subsample is True, as otherwise, c_out = c_in
        """
        super().__init__()
        if not subsample:
            c_out = c_in

        # Network representing F
        self.net = nn.Sequential(
            nn.Conv2d(c_in, c_out, kernel_size=3, padding=1, stride=1 if not subsample else 2, bias=False),  # No bias needed as the Batch Norm handles it
            nn.BatchNorm2d(c_out),
            act_fn(),
            nn.Conv2d(c_out, c_out, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(c_out)
        )

        # 1x1 convolution with stride 2 means we take the upper left value, and transform it to new output size
        self.downsample = nn.Conv2d(c_in, c_out, kernel_size=1, stride=2) if subsample else None
        self.act_fn = act_fn()

    def forward(self, x):
        z = self.net(x)
        if self.downsample is not None:
            x = self.downsample(x)
        out = z + x
        out = self.act_fn(out)
        return out

"""Blok kedua yang akan kita implementasikan adalah blok Pre-Activation ResNet. Untuk ini, kita harus mengubah urutan lapisan di dalam `self.net`, dan tidak menerapkan fungsi aktivasi pada output. Selain itu, operasi downsampling harus menerapkan non-linieritas karena input $x_l$ belum diproses oleh non-linieritas. Oleh karena itu, blok ini akan terlihat seperti berikut:"""

class PreActResNetBlock(nn.Module):

    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):
        """
        Inputs:
            c_in - Number of input features
            act_fn - Activation class constructor (e.g. nn.ReLU)
            subsample - If True, we want to apply a stride inside the block and reduce the output shape by 2 in height and width
            c_out - Number of output features. Note that this is only relevant if subsample is True, as otherwise, c_out = c_in
        """
        super().__init__()
        if not subsample:
            c_out = c_in

        # Network representing F
        self.net = nn.Sequential(
            nn.BatchNorm2d(c_in),
            act_fn(),
            nn.Conv2d(c_in, c_out, kernel_size=3, padding=1, stride=1 if not subsample else 2, bias=False),
            nn.BatchNorm2d(c_out),
            act_fn(),
            nn.Conv2d(c_out, c_out, kernel_size=3, padding=1, bias=False)
        )

        # 1x1 convolution can apply non-linearity as well, but not strictly necessary
        self.downsample = nn.Sequential(
            nn.BatchNorm2d(c_in),
            act_fn(),
            nn.Conv2d(c_in, c_out, kernel_size=1, stride=2, bias=False)
        ) if subsample else None

    def forward(self, x):
        z = self.net(x)
        if self.downsample is not None:
            x = self.downsample(x)
        out = z + x
        return out

"""Secara mirip dengan pemilihan model, kita mendefinisikan sebuah kamus untuk membuat pemetaan dari string ke kelas blok. Kita akan menggunakan nama string sebagai nilai hiperparameter dalam model kita untuk memilih antara blok-blok ResNet. Silakan implementasikan jenis blok ResNet lainnya dan tambahkan ke sini juga."""

resnet_blocks_by_name = {
    "ResNetBlock": ResNetBlock,
    "PreActResNetBlock": PreActResNetBlock
}

"""Arsitektur ResNet secara keseluruhan terdiri dari tumpukan beberapa blok ResNet, yang beberapa di antaranya melakukan downsampling pada input. Ketika membicarakan blok ResNet dalam seluruh jaringan, kita biasanya mengelompokkan mereka berdasarkan bentuk output yang sama. Oleh karena itu, jika kita mengatakan bahwa ResNet memiliki blok `[3,3,3]`, itu berarti kita memiliki 3 grup yang masing-masing terdiri dari 3 blok ResNet, di mana subsampling terjadi pada blok keempat dan ketujuh. ResNet dengan blok `[3,3,3]` pada CIFAR10 divisualisasikan di bawah ini.

<center width="100%"><img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/resnet_notation.svg?raw=1" width="500px"></center>

Tiga grup beroperasi pada resolusi $32\times32$, $16\times16$ dan $8\times8$ secara berturut-turut. Blok-blok yang berwarna oranye menunjukkan blok ResNet dengan downsampling. Notasi yang sama digunakan oleh banyak implementasi lain seperti di [torchvision library](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnet18) dari PyTorch. Dengan demikian, kode kita akan terlihat seperti berikut:
"""

class ResNet(nn.Module):

    def __init__(self, num_classes=10, num_blocks=[3,3,3], c_hidden=[16,32,64], act_fn_name="relu", block_name="ResNetBlock", **kwargs):
        """
        Inputs:
            num_classes - Number of classification outputs (10 for CIFAR10)
            num_blocks - List with the number of ResNet blocks to use. The first block of each group uses downsampling, except the first.
            c_hidden - List with the hidden dimensionalities in the different blocks. Usually multiplied by 2 the deeper we go.
            act_fn_name - Name of the activation function to use, looked up in "act_fn_by_name"
            block_name - Name of the ResNet block, looked up in "resnet_blocks_by_name"
        """
        super().__init__()
        assert block_name in resnet_blocks_by_name
        self.hparams = SimpleNamespace(num_classes=num_classes,
                                       c_hidden=c_hidden,
                                       num_blocks=num_blocks,
                                       act_fn_name=act_fn_name,
                                       act_fn=act_fn_by_name[act_fn_name],
                                       block_class=resnet_blocks_by_name[block_name])
        self._create_network()
        self._init_params()

    def _create_network(self):
        c_hidden = self.hparams.c_hidden

        # A first convolution on the original image to scale up the channel size
        if self.hparams.block_class == PreActResNetBlock: # => Don't apply non-linearity on output
            self.input_net = nn.Sequential(
                nn.Conv2d(3, c_hidden[0], kernel_size=3, padding=1, bias=False)
            )
        else:
            self.input_net = nn.Sequential(
                nn.Conv2d(3, c_hidden[0], kernel_size=3, padding=1, bias=False),
                nn.BatchNorm2d(c_hidden[0]),
                self.hparams.act_fn()
            )

        # Creating the ResNet blocks
        blocks = []
        for block_idx, block_count in enumerate(self.hparams.num_blocks):
            for bc in range(block_count):
                subsample = (bc == 0 and block_idx > 0) # Subsample the first block of each group, except the very first one.
                blocks.append(
                    self.hparams.block_class(c_in=c_hidden[block_idx if not subsample else (block_idx-1)],
                                             act_fn=self.hparams.act_fn,
                                             subsample=subsample,
                                             c_out=c_hidden[block_idx])
                )
        self.blocks = nn.Sequential(*blocks)

        # Mapping to classification output
        self.output_net = nn.Sequential(
            nn.AdaptiveAvgPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(c_hidden[-1], self.hparams.num_classes)
        )

    def _init_params(self):
        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function
        # Fan-out focuses on the gradient distribution, and is commonly used in ResNets
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity=self.hparams.act_fn_name)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.input_net(x)
        x = self.blocks(x)
        x = self.output_net(x)
        return x

"""Untuk menambahkan kelas ResNet ke dalam kamus model (model_dict), kita cukup memperbarui model_dict untuk mencakup pasangan kunci-nilai di mana kuncinya adalah pengenal string untuk model dan nilainya adalah kelas model itu sendiri."""

model_dict["ResNet"] = ResNet

"""

Akhirnya, kita bisa melatih model ResNet kita. Salah satu perbedaan dari pelatihan GoogleNet adalah bahwa kita secara eksplisit menggunakan optimasi SGD dengan Momentum alih-alih Adam. Adam sering menghasilkan akurasi yang sedikit lebih rendah pada ResNet yang sederhana dan dangkal.

Alasan mengapa Adam memiliki kinerja yang lebih buruk dalam konteks ini tidak sepenuhnya jelas, tetapi salah satu penjelasan yang mungkin terkait dengan permukaan kerugian (see [Li et al., 2018](https://arxiv.org/pdf/1712.09913.pdf) for details) dari ResNet. ResNet telah terbukti menghasilkan permukaan kerugian yang lebih halus dibandingkan dengan jaringan tanpa skip connection (figure credit - [Li et al.](https://arxiv.org/pdf/1712.09913.pdf)).

<center width="100%"><img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/resnet_loss_surface.svg?raw=1" style="display: block; margin-left: auto; margin-right: auto;" width="600px"/></center>

Sumbu $x$ dan $y$ menunjukkan proyeksi ruang parameter, sedangkan sumbu $z$ menunjukkan nilai kerugian (loss) yang dicapai oleh berbagai nilai parameter.

Pada permukaan kerugian yang halus seperti di sebelah kanan, kita mungkin tidak memerlukan tingkat pembelajaran adaptif seperti yang disediakan oleh Adam. Sebagai gantinya, Adam dapat terjebak dalam minima lokal, sedangkan SGD cenderung menemukan minima yang lebih luas, yang biasanya memberikan generalisasi yang lebih baik.

Namun, untuk menjawab pertanyaan ini secara mendetail, diperlukan pembahasan tambahan karena ini bukanlah topik yang mudah dijelaskan secara sederhana. Untuk saat ini, kita menyimpulkan: pada arsitektur ResNet, optimizer merupakan hiperparameter penting, dan disarankan untuk mencoba pelatihan dengan Adam dan SGD.

Sekarang, mari kita latih model menggunakan SGD."""

resnet_model, resnet_results = train_model(model_name="ResNet",
                                           model_hparams={"num_classes": 10,
                                                          "c_hidden": [16,32,64],
                                                          "num_blocks": [3,3,3],
                                                          "act_fn_name": "relu"},
                                           optimizer_name="SGD",
                                           optimizer_hparams={"lr": 0.1,
                                                              "momentum": 0.9,
                                                              "weight_decay": 1e-4})

"""Mari kita juga melatih ResNet pre-activation sebagai perbandingan:"""

resnetpreact_model, resnetpreact_results = train_model(model_name="ResNet",
                                                       model_hparams={"num_classes": 10,
                                                                      "c_hidden": [16,32,64],
                                                                      "num_blocks": [3,3,3],
                                                                      "act_fn_name": "relu",
                                                                      "block_name": "PreActResNetBlock"},
                                                       optimizer_name="SGD",
                                                       optimizer_hparams={"lr": 0.1,
                                                                          "momentum": 0.9,
                                                                          "weight_decay": 1e-4},
                                                       save_name="ResNetPreAct")

"""### Tensorboard log

Sama seperti model GoogleNet, kita juga memiliki log TensorBoard untuk model ResNet. Kita dapat membukanya di bawah ini.
"""

# Commented out IPython magic to ensure Python compatibility.
# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH! Feel free to change "ResNet" to "ResNetPreAct"
# %tensorboard --logdir ../saved_models/tutorial5/tensorboards/ResNet/

"""<center width="100%"><img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/tensorboard_screenshot_ResNet.png?raw=1" width="1000px"></center>

Silakan eksplorasi TensorBoard sendiri, termasuk grafik komputasi. Secara umum, kita dapat melihat bahwa dengan SGD, ResNet memiliki kerugian (loss) pelatihan yang lebih tinggi dibandingkan GoogleNet pada tahap awal pelatihan. Namun, setelah mengurangi tingkat pembelajaran (learning rate), model ini mencapai akurasi validasi yang bahkan lebih tinggi. Kami akan membandingkan skor-skor secara rinci di akhir notebook.

## DenseNet

[DenseNet](https://arxiv.org/abs/1608.06993) adalah arsitektur jaringan saraf dalam yang memungkinkan model menjadi sangat dalam dengan pendekatan yang berbeda terhadap koneksi residual. Alih-alih memodelkan perbedaan antar lapisan, DenseNet memanfaatkan koneksi residual sebagai cara untuk menggunakan kembali fitur antar lapisan, sehingga menghilangkan kebutuhan untuk mempelajari peta fitur yang redundan.

Ketika jaringan semakin dalam, model belajar fitur abstrak untuk mengenali pola. Namun, beberapa pola kompleks terdiri dari kombinasi fitur abstrak (misalnya, tangan, wajah) dan fitur tingkat rendah (misalnya, tepi, warna dasar). Untuk menemukan fitur tingkat rendah ini di lapisan dalam, CNN standar harus belajar menyalin peta fitur tersebut, yang menghabiskan banyak parameter yang tidak perlu.

[DenseNet](https://arxiv.org/abs/1608.06993) menawarkan cara yang lebih efisien untuk menggunakan ulang fitur dengan membuat setiap konvolusi bergantung pada semua fitur input sebelumnya, tetapi hanya menambahkan sejumlah kecil filter ke dalamnya. Ilustrasi arsitektur ini dapat dilihat pada gambar di bawah [Hu et al.](https://arxiv.org/abs/1608.06993)):

<center width="100%"><img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/densenet_block.svg?raw=1" style="display: block; margin-left: auto; margin-right: auto;" width="500px"/></center>

Lapisan terakhir, yang disebut sebagai transition layer, bertugas mengurangi dimensi peta fitur baik dalam hal tinggi, lebar, maupun ukuran saluran. Meskipun secara teknis lapisan ini memutus aliran identitas dalam backpropagation, jumlah lapisan ini sangat sedikit dalam satu jaringan sehingga dampaknya terhadap aliran gradien tidak terlalu signifikan.

Implementasi lapisan-lapisan dalam DenseNet dibagi menjadi tiga bagian utama: `DenseLayer`,`DenseBlock`, dan `TransitionLayer`.

`DenseLayer` adalah modul yang mengimplementasikan satu lapisan di dalam dense block. Lapisan ini menerapkan konvolusi 1x1 untuk reduksi dimensi, diikuti dengan konvolusi 3x3. Saluran keluaran dari lapisan ini kemudian dikonkatenasi (digabungkan) dengan saluran asli dan dikembalikan.

Batch Normalization diterapkan sebagai lapisan pertama dari setiap blok. Hal ini memungkinkan aktivasi yang sedikit berbeda untuk fitur yang sama pada berbagai lapisan, tergantung kebutuhan.

Secara keseluruhan, implementasinya adalah sebagai berikut:
"""

class DenseLayer(nn.Module):

    def __init__(self, c_in, bn_size, growth_rate, act_fn):
        """
        Inputs:
            c_in - Number of input channels
            bn_size - Bottleneck size (factor of growth rate) for the output of the 1x1 convolution. Typically between 2 and 4.
            growth_rate - Number of output channels of the 3x3 convolution
            act_fn - Activation class constructor (e.g. nn.ReLU)
        """
        super().__init__()
        self.net = nn.Sequential(
            nn.BatchNorm2d(c_in),
            act_fn(),
            nn.Conv2d(c_in, bn_size * growth_rate, kernel_size=1, bias=False),
            nn.BatchNorm2d(bn_size * growth_rate),
            act_fn(),
            nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)
        )

    def forward(self, x):
        out = self.net(x)
        out = torch.cat([out, x], dim=1)
        return out

"""

Modul `DenseBlock` merangkum beberapa lapisan dense yang diterapkan secara berurutan. Setiap lapisan dense mengambil input berupa input asli yang dikonkatenasi (digabung) dengan peta fitur dari semua lapisan sebelumnya:"""

class DenseBlock(nn.Module):

    def __init__(self, c_in, num_layers, bn_size, growth_rate, act_fn):
        """
        Inputs:
            c_in - Number of input channels
            num_layers - Number of dense layers to apply in the block
            bn_size - Bottleneck size to use in the dense layers
            growth_rate - Growth rate to use in the dense layers
            act_fn - Activation function to use in the dense layers
        """
        super().__init__()
        layers = []
        for layer_idx in range(num_layers):
            layers.append(
                DenseLayer(c_in=c_in + layer_idx * growth_rate, # Input channels are original plus the feature maps from previous layers
                           bn_size=bn_size,
                           growth_rate=growth_rate,
                           act_fn=act_fn)
            )
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        out = self.block(x)
        return out

"""Akhirnya, `TransitionLayer` mengambil output akhir dari sebuah dense block dan mengurangi dimensi saluran (channel) menggunakan konvolusi 1x1. Untuk mengurangi dimensi tinggi dan lebar, kami menggunakan pendekatan yang sedikit berbeda dibandingkan dengan ResNet dan menerapkan average pooling dengan ukuran kernel 2 dan stride 2. Hal ini karena kami tidak memiliki koneksi tambahan ke output yang akan mempertimbangkan seluruh patch 2x2 sebagai pengganti satu nilai. Selain itu, ini lebih efisien dalam penggunaan parameter dibandingkan menggunakan konvolusi 3x3 dengan stride 2. Dengan demikian, lapisan ini diimplementasikan sebagai berikut:"""

class TransitionLayer(nn.Module):

    def __init__(self, c_in, c_out, act_fn):
        super().__init__()
        self.transition = nn.Sequential(
            nn.BatchNorm2d(c_in),
            act_fn(),
            nn.Conv2d(c_in, c_out, kernel_size=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2) # Average the output for each 2x2 pixel group
        )

    def forward(self, x):
        return self.transition(x)

"""Sekarang kita dapat menyatukan semuanya dan membuat DenseNet kita. Untuk menentukan jumlah lapisan, kita menggunakan notasi yang mirip dengan ResNet dan mengirimkan daftar bilangan bulat yang mewakili jumlah lapisan per blok. Setelah setiap blok padat kecuali yang terakhir, kita menerapkan lapisan transisi untuk mengurangi dimensi dengan 2."""

class DenseNet(nn.Module):

    def __init__(self, num_classes=10, num_layers=[6,6,6,6], bn_size=2, growth_rate=16, act_fn_name="relu", **kwargs):
        super().__init__()
        self.hparams = SimpleNamespace(num_classes=num_classes,
                                       num_layers=num_layers,
                                       bn_size=bn_size,
                                       growth_rate=growth_rate,
                                       act_fn_name=act_fn_name,
                                       act_fn=act_fn_by_name[act_fn_name])
        self._create_network()
        self._init_params()

    def _create_network(self):
        c_hidden = self.hparams.growth_rate * self.hparams.bn_size # The start number of hidden channels

        # A first convolution on the original image to scale up the channel size
        self.input_net = nn.Sequential(
            nn.Conv2d(3, c_hidden, kernel_size=3, padding=1) # No batch norm or activation function as done inside the Dense layers
        )

        # Creating the dense blocks, eventually including transition layers
        blocks = []
        for block_idx, num_layers in enumerate(self.hparams.num_layers):
            blocks.append(
                DenseBlock(c_in=c_hidden,
                           num_layers=num_layers,
                           bn_size=self.hparams.bn_size,
                           growth_rate=self.hparams.growth_rate,
                           act_fn=self.hparams.act_fn)
            )
            c_hidden = c_hidden + num_layers * self.hparams.growth_rate # Overall output of the dense block
            if block_idx < len(self.hparams.num_layers)-1: # Don't apply transition layer on last block
                blocks.append(
                    TransitionLayer(c_in=c_hidden,
                                    c_out=c_hidden // 2,
                                    act_fn=self.hparams.act_fn))
                c_hidden = c_hidden // 2

        self.blocks = nn.Sequential(*blocks)

        # Mapping to classification output
        self.output_net = nn.Sequential(
            nn.BatchNorm2d(c_hidden), # The features have not passed a non-linearity until here.
            self.hparams.act_fn(),
            nn.AdaptiveAvgPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(c_hidden, self.hparams.num_classes)
        )

    def _init_params(self):
        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, nonlinearity=self.hparams.act_fn_name)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.input_net(x)
        x = self.blocks(x)
        x = self.output_net(x)
        return x

"""Mari kita tambahkan DenseNet ke dalam kamus model kita:"""

model_dict["DenseNet"] = DenseNet

"""Terakhir, kita melatih jaringan kita. Berbeda dengan ResNet, DenseNet tidak menunjukkan masalah dengan Adam, sehingga kita melatihnya menggunakan optimizer ini. Hyperparameter lainnya dipilih agar menghasilkan jaringan dengan ukuran parameter yang mirip dengan ResNet dan GoogleNet. Secara umum, saat merancang jaringan yang sangat dalam, DenseNet lebih efisien dalam penggunaan parameter dibandingkan dengan ResNet, sambil mencapai kinerja yang serupa atau bahkan lebih baik."""

densenet_model, densenet_results = train_model(model_name="DenseNet",
                                               model_hparams={"num_classes": 10,
                                                              "num_layers": [6,6,6,6],
                                                              "bn_size": 2,
                                                              "growth_rate": 16,
                                                              "act_fn_name": "relu"},
                                               optimizer_name="Adam",
                                               optimizer_hparams={"lr": 1e-3,
                                                                  "weight_decay": 1e-4})

"""### Tensorboard log

Akhirnya, kami juga memiliki TensorBoard lain untuk pelatihan DenseNet. Kami akan melihatnya di bawah:
"""

# Commented out IPython magic to ensure Python compatibility.
# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!
# %tensorboard --logdir ../saved_models/tutorial5/tensorboards/DenseNet/

"""<center width="100%"><img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial5/tensorboard_screenshot_DenseNet.png?raw=1" width="1000px"></center>

Perjalanan keseluruhan dari akurasi validasi dan kehilangan pelatihan mirip dengan pelatihan GoogleNet, yang juga terkait dengan pelatihan jaringan menggunakan Adam. Silakan jelajahi metrik pelatihan tersebut sendiri.

## Conclusion and Comparison

Setelah membahas setiap model secara terpisah dan melatih semuanya, kita akhirnya dapat membandingkannya. Pertama, mari kita atur hasil dari semua model dalam sebuah tabel:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <!-- Some HTML code to increase font size in the following table -->
# <style>
# th {font-size: 120%;}
# td {font-size: 120%;}
# </style>

import tabulate
from IPython.display import display, HTML
all_models = [
    ("GoogleNet", googlenet_results, googlenet_model),
    ("ResNet", resnet_results, resnet_model),
    ("ResNetPreAct", resnetpreact_results, resnetpreact_model),
    ("DenseNet", densenet_results, densenet_model)
]
table = [[model_name,
          f"{100.0*model_results['val']:4.2f}%",
          f"{100.0*model_results['test']:4.2f}%",
          "{:,}".format(sum([np.prod(p.shape) for p in model.parameters()]))]
         for model_name, model_results, model in all_models]
display(HTML(tabulate.tabulate(table, tablefmt='html', headers=["Model", "Val Accuracy", "Test Accuracy", "Num Parameters"])))

"""Pertama-tama, kita melihat bahwa semua model bekerja dengan cukup baik. Model sederhana yang telah Anda implementasikan dalam latihan ini mencapai kinerja yang jauh lebih rendah, yang selain disebabkan oleh jumlah parameter yang lebih rendah, juga disebabkan oleh pilihan desain arsitektur. GoogleNet adalah model yang memperoleh kinerja terendah pada set validasi dan tes, meskipun sangat dekat dengan DenseNet. Pencarian hyperparameter yang tepat untuk semua ukuran saluran di GoogleNet kemungkinan besar akan meningkatkan akurasi model hingga tingkat yang sama, tetapi ini juga memakan biaya mengingat banyaknya hyperparameter. ResNet mengungguli DenseNet dan GoogleNet lebih dari 1% pada set validasi, meskipun ada perbedaan kecil antara kedua versi, yaitu versi asli dan pre-aktivasi. Kita dapat menyimpulkan bahwa untuk jaringan yang dangkal, tempat penerapan fungsi aktivasi tampaknya tidak terlalu penting, meskipun beberapa penelitian melaporkan sebaliknya untuk jaringan yang sangat dalam [He et al.](https://arxiv.org/abs/1603.05027).

Secara umum, kita dapat menyimpulkan bahwa ResNet adalah arsitektur yang sederhana, tetapi sangat kuat. Jika kita menerapkan model-model ini pada tugas yang lebih kompleks dengan gambar yang lebih besar dan lebih banyak lapisan di dalam jaringan, kita kemungkinan besar akan melihat kesenjangan yang lebih besar antara GoogleNet dan arsitektur koneksi-lompatan seperti ResNet dan DenseNet. Perbandingan dengan model-model yang lebih dalam pada CIFAR10, misalnya, dapat ditemukan [di sini](https://github.com/kuangliu/pytorch-cifar). Menariknya, DenseNet mengungguli ResNet asli pada pengaturan mereka tetapi hampir tertinggal di belakang Pre-Activation ResNet. Model terbaik, Dual Path Network ([Chen et. al](https://arxiv.org/abs/1707.01629)), sebenarnya adalah kombinasi dari ResNet dan DenseNet yang menunjukkan bahwa keduanya menawarkan keuntungan yang berbeda.

### Which model should I choose for my task?

Kami telah mengulas empat model yang berbeda. Jadi, model mana yang harus kita pilih jika diberikan tugas baru? Biasanya, memulai dengan ResNet adalah ide yang baik mengingat kinerjanya yang superior pada dataset CIFAR dan implementasinya yang sederhana. Selain itu, untuk jumlah parameter yang telah kita pilih di sini, ResNet adalah yang tercepat karena DenseNet dan GoogleNet memiliki lebih banyak lapisan yang diterapkan secara berurutan dalam implementasi sederhana kita. Namun, jika Anda memiliki tugas yang benar-benar sulit, seperti segmentasi semantik pada gambar HD, varian ResNet dan DenseNet yang lebih kompleks sangat disarankan.

---

[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider -ing our repository.    
[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub.

---
"""